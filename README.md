# What_for_dinner

*This is a record of my UG Y2S1 DSAI proj, which is built with other 3 teamates (when i find their Github account im going to @them)*

*And the aim of this mini proj was to find out the most ***_unpopular_*** movies from a dataset so as a result we can decide*

***What For Dinner***

----------------------------------------------------------------------------------------------------------------- 
## Libraries used:

1. numpy
2. sklearn
3. matplot
4. wordcloud
5. pandas
6. seaborn

----------------------------------------------------------------------------------------------------------------- 
## Results:

Some visualized results are uploaded inside the Pics Folder

(Cus i dont know how to add the image here:( )

### Prediction: 

when the point settled on the diagonal means Prediction fits truth. 
(X-axis is the real score of a movie, Y-axis is the prediction score of a movie)

### Anomaly:

the yellow points are normal points (i.e. whose score fits our model's prediction); the purple ones means anomaly occurs.



## Development Log:

### Sample Collection

✅10% for coming up with an interesting problem based on the dataset
- Which dataset to work on?

        https://www.kaggle.com/syedmubarak/netflix-dataset-latest-2021

- What is the objective/ problem that aiming to solve? 

  Find out those movies are not recommended for a student who is a movie lover

- What type of **ML project (_regression/classification/clustering/anomaly_)** have you done?

1. ***Classification***: Popularity of several genres of movie

2. ***Clustering***: Popularity of several genres of movie

3. ***Anomaly***: High-rated movies with low box office, Low-rated movies with high box office

4. ***Regression***:Relationship between rating and popularity? Are the scores and popularity related to the director, writer and actor of the movie

----------------------------------------------------------------------------------------------------------------- 
### Data Preparation - arrange raw data for analysis

✅10% for preparing the dataset to suit your specific problem definition

  1. Clean dataset
  
  2. Structure data
  
  3. Remove outliers

----------------------------------------------------------------------------------------------------------------- 
### Exploratory Data Analysis - basic insight from data
✅5% for exploratory data analysis

1. Data mining 

2. Compute relevant vital statistics


----------------------------------------------------------------------------------------------------------------- 
### Analytic Visualization
✅5% for visualization to understand the data

1. Visually represent statistics

2. Highlight “interesting” traits i.e. 6 dimensions: ***accuracy, completeness, consistency, timeliness, validity, and uniqueness***


----------------------------------------------------------------------------------------------------------------- 
### Algorithmic Optimization (Machine learning)
✅20% for the use of data science / machine learning to solve the problem

1. Problem solving 

2. Reduce error

3. Generalize the Algorithms

4. Information Presentation - draw conclusion 

5. Present Descriptive Analysis & Inferential Analysis

6. Estimate the confidence


----------------------------------------------------------------------------------------------------------------- 
## Requirements of this Project


new DS/ML model for regression, classification, clustering or anomaly detection

new visualization tool (like Plotly)

new technique for data preparation (like resampling)

explore additional data (to add to the given datasets).




----------------------------------------------------------------------------------------------------------------- 

<details><summary>CLICK ME</summary>

<p>

-------------------EE0005_What_For_Dinner_Grp_Mini_Proj.-----------------------------

To start with, we want to say THANK YOU to our dear prof, Mr. Law, and we hope can get good grades for this project <3<3<3 XDXDXD

--------------------------------------------------------------------------------------------

and here is what we wanna say about this project:

After thousands or million (just kidding) times of errors, we finally finished this project, from the choosing of topic, to devide the task for each one in the group, we experienced a really really looooooooooooooong time.

To be honest, we decided to do the food dataset first, after all we are the 'WHAT FOR DINNER' group XD. however, because of afraid of other groups, what if they do the same dataset, and we cannot do our job as well as them... then we choose this topic.
BUT! here i wanna say to my group mates: we are still doing our best! XD we really did a great job didn't we? 

And, in the point of view of mine, i think Qintian and Ziyi, they are really good teammates. because at the beginning they gave out some really helpful ideas.
(and hengji and i we are somehow “划水”in chinese, which means not really working on it) after they take the idea of doing what, and how to make it, we start to work on it (not 划水 this time XD)

and they really did a great work! so much results and formulars and graphs, they really did alot.

and there are also some kinds of problems during the programing.
such as the compatibility on different devices, we have intel-chip mac, M1 chip mac, AMD chip windows PC and intel chip windows laptop, it is sooooooooo hard to deal with teh compatibility..... sometimes the csv or excel read on the M1 mac is different from the one read on the Windows pc... 
soooooo hard to deal with this part...

and the clustering part is still have some bugs)
for example, we have 3 clusters, however the index of the clusters are different each time... but the code was wrote based on one times result. so i tried to refresh it time by time till the index is in a correct order... 0.0
this is really a happy and tight time... and gain lots in the programming part of python.

last, the conbining part.. makes my heart break... we have the same named dataframe but not the same content inside... so its really a big problem in combining the code...
next time if i have a chance to do such a job, i will use co-lab first....

-----------------------------------------------------------------------------------------------
(if you really read this, thank you for your time! and we really hope to get a good grade lah XDXDXD)
(keep safe stay healthy!) <3

--------------------------------------------------------------------------------------------------


</p>

</details>
